# ğŸ§­ GridWorld â€” SARSA vs n-Step Double Q-Learning

## ğŸ“˜ Overview
This project implements and compares **SARSA** and **n-Step Double Q-Learning** algorithms on a custom **GridWorld** environment.  
It focuses on exploring reinforcement learning fundamentals such as temporal-difference learning, on-policy vs off-policy updates, Q-value stability, and explorationâ€“exploitation tradeoffs.

Both algorithms are trained and evaluated using identical configurations for fairness, with Optuna-based hyperparameter tuning and rich visualizations for comparison.

---

## ğŸ¯ Objectives
- Implement SARSA and n-Step Double Q-Learning agents from scratch.  
- Tune hyperparameters automatically using **Optuna**.  
- Compare convergence speed, stability, and long-term reward.  
- Visualize epsilon decay, total reward per episode, and greedy policy evaluation.  
- Identify the **optimal step size (n)** for Double Q-Learning.

---

## ğŸ§© Environment
**Environment Name:** `YavarEnv` (Custom GridWorld)  
**Type:** Deterministic 2D grid  
**Goal:** Reach terminal state with maximum cumulative reward  
**State Representation:** (row, column) coordinates  
**Action Space:** Up, Down, Left, Right  
**Rewards:**
- +1 for reaching goal  
- -1 for invalid moves or collisions  
- 0 otherwise  

---

## âš™ï¸ Algorithms Implemented

### ğŸ”¹ SARSA (Stateâ€“Actionâ€“Rewardâ€“Stateâ€“Action)
On-policy TD control algorithm that updates Q-values based on the next action actually taken by the policy.

**Update Rule:**
\[
Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma Q(s',a') - Q(s,a)]
\]

### ğŸ”¹ n-Step Double Q-Learning
An off-policy method using two Q-tables to minimize overestimation bias and multi-step returns for faster learning.

**Key Features:**
- Two Q-tables (`Q1`, `Q2`) updated alternately.  
- Uses *n-step bootstrapping* to incorporate multi-step returns.  
- Tuning over **n** (1â€“5) to find optimal tradeoff between bias and variance.

---

## âš™ï¸ Hyperparameter Tuning
**Library:** Optuna  
**Search Space:**
| Parameter | Range / Values |
|------------|----------------|
| Discount factor (Î³) | [0.8, 0.99] |
| Exploration rate (Îµ) | [0.05, 1.0] |
| Îµ-decay | [0.90, 0.999] |
| Learning rate (Î±) | [0.001, 0.1] |
| n (for Double Q-Learning) | [1, 5] |
| Episodes | 100â€“500 |
| Max timesteps | 100â€“300 |

**Optimization Metric:** Highest average reward over last 10 episodes.

**Best Results:**
- Optimal n = **3**  
- Tuned hyperparameters improve both reward and convergence speed.

---

## ğŸ“Š Results Summary
| Algorithm | Optimal n | Final Avg Reward | Convergence | Stability |
|------------|------------|------------------|--------------|------------|
| SARSA | â€” | 0.88 | Moderate | Stable |
| n-Step Double Q-Learning | 3 | 0.92 | Faster | High |

**Last 10 Episode Rewards (example):**  
SARSA â†’ [0.79, 0.80, 0.85, 0.89, 0.90, 0.91, 0.92, 0.92, 0.92, 0.93]  
n-Step Double Q-Learning â†’ [0.83, 0.86, 0.88, 0.91, 0.93, 0.94, 0.94, 0.95, 0.95, 0.96]

---

## ğŸ“ˆ Visualizations
1. **Reward per Episode:** Learning curves for SARSA and Double Q-Learning.  
2. **Epsilon Decay:** Demonstrates reduced exploration over time.  
3. **Greedy Policy Evaluation:** Final learned policies visualized on the grid.  
4. **Comparative Performance:** Overlay plot â€” SARSA vs Double Q-Learning (n = 3).

---

## ğŸ§© Insights

- n-Step Double Q-Learning significantly reduces overestimation bias by decoupling action selection and evaluation.
- The addition of n-step returns accelerates convergence.
- SARSA, being on-policy, tends to be more stable but learns slower.
- The tuned parameters demonstrate how critical learning rate and Îµ-decay scheduling are to convergence.

---

##  Plots
- Total Reward per Episode
- Epsilon Decay Curve
- SARSA vs n-Step Double Q-Learning

---

## ğŸ§ª Usage

```bash
### 1ï¸âƒ£ Clone Repository

git clone https://github.com/yavar29/ML_Projects.git
cd "Deep Learning Models/GridWorld_RL"

### 2ï¸âƒ£ Install Dependencies
pip install numpy matplotlib optuna

### 3ï¸âƒ£ Run Training & Comparison
python gridworld_main.py


---

## Suggested Directory Structure
GridWorld_RL/
â”œâ”€â”€ gridworld_env.py            # Custom environment class
â”œâ”€â”€ sarsa.py                    # SARSA implementation
â”œâ”€â”€ double_q_nstep.py           # n-Step Double Q-Learning
â”œâ”€â”€ tuning.py                   # Optuna hyperparameter optimization
â”œâ”€â”€ plots.py                    # Visualization utilities
â”œâ”€â”€ gridworld_main.py           # Main entry point
â””â”€â”€ README.md

---





